# -*- coding: utf-8 -*-
"""delhi flats.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vH3-taWz6-1Xb207jhM1Q-aZLp63A2J3

# Importing & Installing Libraries
"""

#installing required library
!pip install sweetviz

!pip install category_encoders

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import plotly.express as px
import sweetviz
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.utils.validation import check_is_fitted
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_percentage_error, roc_auc_score
from category_encoders import OneHotEncoder
from ipywidgets import Dropdown, FloatSlider, IntSlider, interact
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor

"""# I. Preparing Data

##Importing the data
"""

def wrangle(filepath):
    #read csv file into df
    df = pd.read_csv(filepath)

    #to subset properties in capital fed
    mask_flat = df["type_of_building"] == "Flat"
    
    
    #remove outliers by surface area 
    low,high =df["area"].quantile([0.1,0.9])
    mask_area = df["area"].between(low,high)

    #drop columns wiith null values greater than 50 %
    for column in df:
        if round(df[column].isnull().sum() / len(df),1) >= 0.5:
            df.drop(columns=[column], inplace=True)
    
    
    #drop leaky colums
    df.drop(columns=['Price_sqft'],inplace=True)

    #drop low and high cardinality categorical variables
    df.drop(columns=["Unnamed: 0", "type_of_building", "Address", "desc", "Bathrooms","Status", "neworold"], inplace=True)

    #drop duplicate rows
    df.drop_duplicates(keep=False, inplace=True)

    df = df[mask_flat & mask_area]

    
    return df

"""##EDA"""

df.isnull().sum() / len(df)

df = wrangle("drive/MyDrive/delhi housing/Delhi_v2.csv")
print("df shape:", df.shape)
df.head()

df.info()

#autoEDA using sweetviz
autoEDA = sweetviz.analyze(df)
autoEDA.show_notebook()

plt.hist(df["area"])
plt.xlabel("Area")
plt.title("Distribution of Flat Sizes");

# Build scatter plot
plt.scatter(x=df["area"], y = df["price"])


# Label axes
plt.xlabel("Area")
plt.ylabel("Price")

# Add title
plt.title("Delhi")

# Create 3D scatter plot
fig = px.scatter_3d(
    df,
    x="latitude",
    y="longitude",
    z="price",
    labels={"longitude": "longitude", "latitude": "latitude", "price": "price"},
    width=600,
    height=500,
)

# Refine formatting
fig.update_traces(
    marker={"size": 4, "line": {"width": 2, "color": "DarkSlateGrey"}},
    selector={"mode": "markers"},
)

# Display figure
fig.show()

# Plot Mapbox location and price
fig = px.scatter_mapbox(
    df,  # Our DataFrame
    lat = "latitude",
    lon ="longitude",
    width=600,  # Width of map
    height=600,  # Height of map
    color="price",
    hover_data=["price"],  # Display price when hovering mouse over house
)

fig.update_layout(mapbox_style="open-street-map")



fig.show()

df.describe()["area"]

df.nunique()

corr = df.select_dtypes("number").drop(columns="price").corr()
sns.heatmap(corr)

corr

df[["Bathrooms","price"]].corr()

df[["area","price"]].corr()

#checking for null values
display(df.isna().sum()) #no null values

"""##Split the Data"""

target = "price"
X = df.drop(columns = target)
y = df[target]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size = 0.2, random_state = 42 
)

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

X_train.info()

"""### Scaling the data"""

X1 = X
y1 = y

X_train1, X_test1, y_train1, y_test1 = train_test_split(
    X1, y1, test_size = 0.2, random_state = 42 
)

#defining scaling object
scaler = StandardScaler()

X_train1 = scaler.fit_transform(X_train1)
X_test1 = scaler.transform(X_test1)

"""# II. MODELS"""

r2_scores_train = {}
r2_scores_test = {}

"""## Baseline"""

y_mean = y_train.mean()
y_pred_baseline = [y_mean] * len(y_train)
print("Mean flat price:", round(y_mean,2))

print("Baseline MAE:", round(mean_absolute_error(y_train, y_pred_baseline), 2))

r2_scores_train.update({"Baseline": r2_score(y_train, y_pred_baseline)})
r2_scores_test.update({"Baseline": r2_score(y_test, [y_mean] * len(y_test))})

"""## Linear Regression"""

model_linear = make_pipeline(
    SimpleImputer(),
    LinearRegression()
)

model_linear.fit(X_train, y_train)

y_pred_training_linear = model_linear.predict(X_train)

print("Training MAE:", mean_absolute_error(y_train, y_pred_training_linear))

y_pred_test_linear = pd.Series(model_linear.predict(X_test))
print("Testing MAE:", mean_absolute_error(y_test, y_pred_test_linear))

r2_scores_train.update({"Linear": r2_score(y_train, y_pred_training_linear)})
r2_scores_test.update({"Linear": r2_score(y_test, y_pred_test_linear)})

"""## Ridge"""

model_ridge = make_pipeline(
    SimpleImputer(),
    Ridge(alpha=0.7)
)
model_ridge.fit(X_train, y_train)

"""### on unscaled data"""

y_pred_training = model_ridge.predict(X_train)

print("Training MAE:", mean_absolute_error(y_train, y_pred_training))

y_pred_test = pd.Series(model_ridge.predict(X_test))

y_pred_test.head()

print("Testing MAE:", mean_absolute_error(y_test, y_pred_test))

r2_score(y_test, y_pred_test)

r2_score(y_train, y_pred_training)

r2_scores_train.update({"Ridge": r2_score(y_train, y_pred_training)})
r2_scores_test.update({"Ridge": r2_score(y_test, y_pred_test) })

"""### on scaled data"""

model_ridge.fit(X_train1, y_train1)

y_pred_training1 = model_ridge.predict(X_train1)

print("Training MAE1:", mean_absolute_error(y_train1, y_pred_training1))

y_pred_test1 = pd.Series(model_ridge.predict(X_test1))

print("Testing MAE1:", mean_absolute_error(y_test1, y_pred_test1))

r2_score(y_train1, y_pred_training1)

r2_score(y_test1, y_pred_test1)



"""##Lasso"""

model_l = make_pipeline(
    SimpleImputer(),
    Lasso(.15)
)

model_l.fit(X_train, y_train)

y_pred_training_l = model_l.predict(X_train)

print("Training MAE:", mean_absolute_error(y_train, y_pred_training_l))

y_pred_test_l = pd.Series(model_l.predict(X_test))

r2_scores_train.update({"Lasso": r2_score(y_train, y_pred_training_l)})
r2_scores_test.update({"Lasso": r2_score(y_test, y_pred_test_l)})

"""##RF"""

from sklearn.ensemble import RandomForestRegressor

model_rf = make_pipeline(
    SimpleImputer(),
    RandomForestRegressor(random_state=69)

)

model_rf.fit(X_train, y_train)

y_pred_training_rf = model_rf.predict(X_train)

print("Training MAE:", mean_absolute_error(y_train, y_pred_training_rf))

y_pred_test_rf = pd.Series(model_rf.predict(X_test))

r2_scores_train.update({"RF": r2_score(y_train, y_pred_training_rf)})
r2_scores_test.update({"RF": r2_score(y_test, y_pred_test_rf)})

"""##Decision Tree"""

model_dt = make_pipeline(
    SimpleImputer(),
    DecisionTreeRegressor(random_state=42)

)

y_pred_training_dt = model_dt.predict(X_train1)

print("Training MAE:", mean_absolute_error(y_train, y_pred_training_dt))

y_pred_test_dt = pd.Series(model_dt.predict(X_test))

print("Testing MAE:", mean_absolute_error(y_test, y_pred_test_dt))

r2_score(y_test, y_pred_test_dt)

y_pred_test_dt = pd.Series(model_dt.predict(X_test1))

print("Testing MAE:", mean_absolute_error(y_test1, y_pred_test_dt))

r2_score(y_test1, y_pred_test_dt)

r2_scores_train.update({"DT": r2_score(y_train, y_pred_training_dt)})
r2_scores_test.update({"DT": r2_score(y_test, y_pred_test_dt)})

"""## BEST MODEL"""

del r2_scores_train['Baseline']

del r2_scores_test['Baseline']

r2_scores_train

r2_scores_test

best_model = max(r2_scores_test, key= lambda x: r2_scores_test[x])
best_model

"""##Exporting best model that is RandomForest (RF)"""

import pickle
pkl_filename = "pickle_model_best.pkl"
with open(pkl_filename, 'wb') as file:
    pickle.dump(model_rf, file)

"""# III. Communicating Results"""

def make_prediction(area, latitude, longitude, Bedrooms,Balcony):
    
    data = {
        "area":area,
        "latitude" :latitude,
        "longitude":longitude,
        "Bedrooms": Bedrooms,
        "Balcony":Balcony
        

        
    }
    df = pd.DataFrame(data, index=[0])
    
    prediction = model_rf.predict(df).round(2)[0]
    return f"Predicted Flat price: INR {prediction}"

minValues = df.min()
minValues

X_train

X_train = pd.DataFrame(X_train, columns = ['area','latitude','longitude','Bedrooms','Balcony'])

interact(
    make_prediction,
    area=IntSlider(
        min=X_train["area"].min(),
        max=X_train["area"].max(),
        value=X_train["area"].mean(),
    ),
    latitude=FloatSlider(
        min=X_train["latitude"].min(),
        max=X_train["latitude"].max(),
        step=0.01,
        value=X_train["latitude"].mean(),
    ),
    longitude=FloatSlider(
        min=X_train["longitude"].min(),
        max=X_train["longitude"].max(),
        step=0.01,
        value=X_train["longitude"].mean(),
    ),
    Bedrooms=IntSlider(
        min=X_train["Bedrooms"].min().round(),
        max=X_train["Bedrooms"].max().round(),
        value=X_train["Bedrooms"].mean().round(),
    ),
    Balcony=IntSlider(
        min=X_train["Balcony"].min().round(),
        max=X_train["Balcony"].max().round(),
        value=X_train["Balcony"].mean().round(),
    )
);

